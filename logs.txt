* 
* ==> Audit <==
* |---------|----------------|----------|--------------|---------|---------------------|---------------------|
| Command |      Args      | Profile  |     User     | Version |     Start Time      |      End Time       |
|---------|----------------|----------|--------------|---------|---------------------|---------------------|
| service | phpfpm-nginx   | minikube | hoangphongle | v1.32.0 | 29 Dec 23 13:56 +07 |                     |
| service | phpfpm-nginx   | minikube | hoangphongle | v1.32.0 | 29 Dec 23 14:30 +07 |                     |
| service | phpfpm-nginx   | minikube | hoangphongle | v1.32.0 | 29 Dec 23 14:37 +07 |                     |
| start   |                | minikube | hoangphongle | v1.32.0 | 29 Dec 23 14:37 +07 | 29 Dec 23 14:41 +07 |
| service | phpfpm-nginx   | minikube | hoangphongle | v1.32.0 | 29 Dec 23 14:49 +07 |                     |
| kubectl | -- get pods -A | minikube | hoangphongle | v1.32.0 | 29 Dec 23 14:49 +07 | 29 Dec 23 14:50 +07 |
| service | phpfpm-nginx   | minikube | hoangphongle | v1.32.0 | 29 Dec 23 14:55 +07 |                     |
|---------|----------------|----------|--------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/29 14:37:59
Running on machine: 192
Binary: Built with gc go1.21.5 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1229 14:37:59.353973   46632 out.go:296] Setting OutFile to fd 1 ...
I1229 14:37:59.354288   46632 out.go:348] isatty.IsTerminal(1) = true
I1229 14:37:59.354292   46632 out.go:309] Setting ErrFile to fd 2...
I1229 14:37:59.354298   46632 out.go:348] isatty.IsTerminal(2) = true
I1229 14:37:59.354534   46632 root.go:338] Updating PATH: /Users/hoangphongle/.minikube/bin
W1229 14:37:59.354674   46632 root.go:314] Error reading config file at /Users/hoangphongle/.minikube/config/config.json: open /Users/hoangphongle/.minikube/config/config.json: no such file or directory
I1229 14:37:59.357248   46632 out.go:303] Setting JSON to false
I1229 14:37:59.398986   46632 start.go:128] hostinfo: {"hostname":"192.168.2.9","uptime":24185,"bootTime":1703811294,"procs":389,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"10.15.7","kernelVersion":"19.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"65fc834e-1eb2-5ced-9c65-a5deaf6091e8"}
W1229 14:37:59.399099   46632 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1229 14:37:59.400806   46632 out.go:177] ðŸ˜„  minikube v1.32.0 on Darwin 10.15.7
W1229 14:37:59.406477   46632 preload.go:295] Failed to list preload files: open /Users/hoangphongle/.minikube/cache/preloaded-tarball: no such file or directory
I1229 14:37:59.407096   46632 notify.go:220] Checking for updates...
I1229 14:37:59.407909   46632 driver.go:378] Setting default libvirt URI to qemu:///system
I1229 14:37:59.408383   46632 global.go:111] Querying for installed drivers using PATH=/Users/hoangphongle/.minikube/bin:/opt/anaconda3/bin:/opt/anaconda3/condabin:/anaconda3/bin:/Users/hoangphongle/.nvm/versions/node/v18.16.0/bin:/Library/Frameworks/Python.framework/Versions/3.6/bin:/Library/Frameworks/Python.framework/Versions/3.6/bin:/Users/hoangphongle/.pyenv/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/Applications/XAMPP/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/usr/local/git/bin:/Applications/Postgres.app/Contents/Versions/latest/bin:/usr/local/mongodb/bin
I1229 14:37:59.741760   46632 docker.go:122] docker version: linux-20.10.17:Docker Desktop 4.10.1 (82475)
I1229 14:37:59.742025   46632 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1229 14:38:00.490288   46632 info.go:266] docker info: {ID:55WL:W7WZ:6WCE:2LTS:44HE:KOUX:SHEY:QGET:HVLV:KDCB:47M2:MRCD Containers:29 ContainersRunning:27 ContainersPaused:0 ContainersStopped:2 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:246 OomKillDisable:false NGoroutines:217 SystemTime:2023-12-29 07:37:59.844659395 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125892608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Err:failed to fetch metadata: fork/exec /Users/hoangphongle/.docker/cli-plugins/docker-compose: no such file or directory Name:compose Path:/Users/hoangphongle/.docker/cli-plugins/docker-compose ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose]] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.7] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1229 14:38:00.490589   46632 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1229 14:38:00.495091   46632 global.go:122] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1229 14:38:00.495164   46632 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1229 14:38:00.524923   46632 global.go:122] hyperkit default: true priority: 8, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1229 14:38:00.525229   46632 global.go:122] parallels default: true priority: 7, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1229 14:38:00.528073   46632 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
W1229 14:38:00.535769   46632 virtualbox.go:101] unable to get virtualbox version, returned: <nil>
I1229 14:38:00.535821   46632 global.go:122] virtualbox default: true priority: 6, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:"/usr/local/bin/VBoxManage --version" returned: <nil> Reason: Fix:Restart VirtualBox, or upgrade to the latest version of VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1229 14:38:00.536077   46632 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1229 14:38:00.536119   46632 driver.go:313] not recommending "ssh" due to default: false
I1229 14:38:00.536126   46632 driver.go:308] not recommending "virtualbox" due to health: "/usr/local/bin/VBoxManage --version" returned: <nil>
I1229 14:38:00.536162   46632 driver.go:348] Picked: docker
I1229 14:38:00.536181   46632 driver.go:349] Alternatives: [hyperkit parallels ssh]
I1229 14:38:00.536189   46632 driver.go:350] Rejects: [podman qemu2 virtualbox vmware]
I1229 14:38:00.538887   46632 out.go:177] âœ¨  Automatically selected the docker driver. Other choices: hyperkit, parallels, ssh
I1229 14:38:00.540758   46632 start.go:298] selected driver: docker
I1229 14:38:00.540782   46632 start.go:902] validating driver "docker" against <nil>
I1229 14:38:00.540805   46632 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1229 14:38:00.541664   46632 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1229 14:38:00.779199   46632 info.go:266] docker info: {ID:55WL:W7WZ:6WCE:2LTS:44HE:KOUX:SHEY:QGET:HVLV:KDCB:47M2:MRCD Containers:29 ContainersRunning:27 ContainersPaused:0 ContainersStopped:2 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:260 OomKillDisable:false NGoroutines:246 SystemTime:2023-12-29 07:38:00.662985376 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125892608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Err:failed to fetch metadata: fork/exec /Users/hoangphongle/.docker/cli-plugins/docker-compose: no such file or directory Name:compose Path:/Users/hoangphongle/.docker/cli-plugins/docker-compose ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose]] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.7] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1229 14:38:00.779615   46632 start_flags.go:309] no existing cluster config was found, will generate one from the flags 
I1229 14:38:00.781226   46632 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=8192MB, container=3934MB
I1229 14:38:00.785447   46632 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I1229 14:38:00.786622   46632 out.go:177] ðŸ“Œ  Using Docker Desktop driver with root privileges
I1229 14:38:00.788623   46632 cni.go:84] Creating CNI manager for ""
I1229 14:38:00.788844   46632 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1229 14:38:00.788858   46632 start_flags.go:318] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1229 14:38:00.788870   46632 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1229 14:38:00.791370   46632 out.go:177] ðŸ‘  Starting control plane node minikube in cluster minikube
I1229 14:38:00.795530   46632 cache.go:121] Beginning downloading kic base image for docker with docker
I1229 14:38:00.797333   46632 out.go:177] ðŸšœ  Pulling base image ...
I1229 14:38:00.800904   46632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1229 14:38:00.801314   46632 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1229 14:38:01.016070   46632 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1229 14:38:01.017673   46632 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1229 14:38:01.018242   46632 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1229 14:38:01.029788   46632 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1229 14:38:01.029814   46632 cache.go:56] Caching tarball of preloaded images
I1229 14:38:01.030230   46632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1229 14:38:01.031940   46632 out.go:177] ðŸ’¾  Downloading Kubernetes v1.28.3 preload ...
I1229 14:38:01.035094   46632 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I1229 14:38:01.327107   46632 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4?checksum=md5:82104bbf889ff8b69d5c141ce86c05ac -> /Users/hoangphongle/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1229 14:38:43.920243   46632 preload.go:249] saving checksum for preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I1229 14:38:43.920443   46632 preload.go:256] verifying checksum of /Users/hoangphongle/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I1229 14:38:46.649868   46632 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1229 14:38:46.650668   46632 profile.go:148] Saving config to /Users/hoangphongle/.minikube/profiles/minikube/config.json ...
I1229 14:38:46.650728   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/config.json: {Name:mk594bf0d6e4d3c35a4b7f7e8fcdbdb7ccf4a766 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:39:29.436755   46632 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I1229 14:39:29.436773   46632 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 from local cache
I1229 14:40:34.470957   46632 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 from cached tarball
I1229 14:40:34.471507   46632 cache.go:194] Successfully downloaded all kic artifacts
I1229 14:40:34.472163   46632 start.go:365] acquiring machines lock for minikube: {Name:mkb73eacbaa179aa5754cbf8d7ba4c1e7b3bf0b5 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1229 14:40:34.472922   46632 start.go:369] acquired machines lock for "minikube" in 622.838Âµs
I1229 14:40:34.473582   46632 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1229 14:40:34.474008   46632 start.go:125] createHost starting for "" (driver="docker")
I1229 14:40:34.478108   46632 out.go:204] ðŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
I1229 14:40:34.480087   46632 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1229 14:40:34.480834   46632 client.go:168] LocalClient.Create starting
I1229 14:40:34.481382   46632 main.go:141] libmachine: Creating CA: /Users/hoangphongle/.minikube/certs/ca.pem
I1229 14:40:34.936571   46632 main.go:141] libmachine: Creating client certificate: /Users/hoangphongle/.minikube/certs/cert.pem
I1229 14:40:35.352968   46632 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1229 14:40:35.520905   46632 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1229 14:40:35.521454   46632 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I1229 14:40:35.521499   46632 cli_runner.go:164] Run: docker network inspect minikube
W1229 14:40:35.655825   46632 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1229 14:40:35.655881   46632 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1229 14:40:35.655914   46632 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1229 14:40:35.656278   46632 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1229 14:40:35.800690   46632 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc01c398210}
I1229 14:40:35.801020   46632 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1229 14:40:35.801300   46632 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1229 14:40:36.743820   46632 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1229 14:40:36.744258   46632 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1229 14:40:36.745243   46632 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1229 14:40:36.969211   46632 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1229 14:40:37.107289   46632 oci.go:103] Successfully created a docker volume minikube
I1229 14:40:37.107637   46632 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib
I1229 14:40:39.059406   46632 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib: (1.951636582s)
I1229 14:40:39.059443   46632 oci.go:107] Successfully prepared a docker volume minikube
I1229 14:40:39.059455   46632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1229 14:40:39.059786   46632 kic.go:194] Starting extracting preloaded images to volume ...
I1229 14:40:39.060196   46632 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/hoangphongle/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir
I1229 14:41:02.801261   46632 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/hoangphongle/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir: (23.740111714s)
I1229 14:41:02.801374   46632 kic.go:203] duration metric: took 23.741153 seconds to extract preloaded images to volume
I1229 14:41:02.801776   46632 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1229 14:41:03.830772   46632 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.028657674s)
I1229 14:41:03.831403   46632 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0
I1229 14:41:05.171847   46632 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0: (1.340274076s)
I1229 14:41:05.172405   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1229 14:41:05.421125   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1229 14:41:05.652060   46632 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1229 14:41:06.132436   46632 oci.go:144] the created container "minikube" has a running status.
I1229 14:41:06.132716   46632 kic.go:225] Creating ssh key for kic: /Users/hoangphongle/.minikube/machines/minikube/id_rsa...
I1229 14:41:06.845353   46632 kic_runner.go:191] docker (temp): /Users/hoangphongle/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1229 14:41:07.193748   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1229 14:41:07.422599   46632 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1229 14:41:07.422641   46632 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1229 14:41:07.766450   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1229 14:41:07.917756   46632 machine.go:88] provisioning docker machine ...
I1229 14:41:07.918544   46632 ubuntu.go:169] provisioning hostname "minikube"
I1229 14:41:07.918982   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:08.114649   46632 main.go:141] libmachine: Using SSH client type: native
I1229 14:41:08.118433   46632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100409e00] 0x10040cae0 <nil>  [] 0s} 127.0.0.1 54619 <nil> <nil>}
I1229 14:41:08.118458   46632 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1229 14:41:08.403168   46632 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1229 14:41:08.403794   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:08.555514   46632 main.go:141] libmachine: Using SSH client type: native
I1229 14:41:08.556174   46632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100409e00] 0x10040cae0 <nil>  [] 0s} 127.0.0.1 54619 <nil> <nil>}
I1229 14:41:08.556212   46632 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1229 14:41:08.840979   46632 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1229 14:41:08.841007   46632 ubuntu.go:175] set auth options {CertDir:/Users/hoangphongle/.minikube CaCertPath:/Users/hoangphongle/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/hoangphongle/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/hoangphongle/.minikube/machines/server.pem ServerKeyPath:/Users/hoangphongle/.minikube/machines/server-key.pem ClientKeyPath:/Users/hoangphongle/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/hoangphongle/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/hoangphongle/.minikube}
I1229 14:41:08.841059   46632 ubuntu.go:177] setting up certificates
I1229 14:41:08.841270   46632 provision.go:83] configureAuth start
I1229 14:41:08.841585   46632 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1229 14:41:08.991752   46632 provision.go:138] copyHostCerts
I1229 14:41:08.992310   46632 exec_runner.go:151] cp: /Users/hoangphongle/.minikube/certs/ca.pem --> /Users/hoangphongle/.minikube/ca.pem (1094 bytes)
I1229 14:41:08.993605   46632 exec_runner.go:151] cp: /Users/hoangphongle/.minikube/certs/cert.pem --> /Users/hoangphongle/.minikube/cert.pem (1139 bytes)
I1229 14:41:08.994444   46632 exec_runner.go:151] cp: /Users/hoangphongle/.minikube/certs/key.pem --> /Users/hoangphongle/.minikube/key.pem (1679 bytes)
I1229 14:41:08.995040   46632 provision.go:112] generating server cert: /Users/hoangphongle/.minikube/machines/server.pem ca-key=/Users/hoangphongle/.minikube/certs/ca.pem private-key=/Users/hoangphongle/.minikube/certs/ca-key.pem org=hoangphongle.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1229 14:41:09.260483   46632 provision.go:172] copyRemoteCerts
I1229 14:41:09.261653   46632 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1229 14:41:09.261795   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:09.384138   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:09.498974   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1229 14:41:09.558514   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I1229 14:41:09.617072   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/machines/server.pem --> /etc/docker/server.pem (1216 bytes)
I1229 14:41:09.673336   46632 provision.go:86] duration metric: configureAuth took 831.741275ms
I1229 14:41:09.673357   46632 ubuntu.go:193] setting minikube options for container-runtime
I1229 14:41:09.674032   46632 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1229 14:41:09.674203   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:09.807993   46632 main.go:141] libmachine: Using SSH client type: native
I1229 14:41:09.808633   46632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100409e00] 0x10040cae0 <nil>  [] 0s} 127.0.0.1 54619 <nil> <nil>}
I1229 14:41:09.808647   46632 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1229 14:41:09.971422   46632 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1229 14:41:09.971439   46632 ubuntu.go:71] root file system type: overlay
I1229 14:41:09.971979   46632 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1229 14:41:09.972234   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:10.107697   46632 main.go:141] libmachine: Using SSH client type: native
I1229 14:41:10.108278   46632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100409e00] 0x10040cae0 <nil>  [] 0s} 127.0.0.1 54619 <nil> <nil>}
I1229 14:41:10.108348   46632 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1229 14:41:10.314242   46632 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1229 14:41:10.314762   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:10.453336   46632 main.go:141] libmachine: Using SSH client type: native
I1229 14:41:10.453858   46632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100409e00] 0x10040cae0 <nil>  [] 0s} 127.0.0.1 54619 <nil> <nil>}
I1229 14:41:10.453882   46632 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1229 14:41:11.761528   46632 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-12-29 07:41:10.323274813 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1229 14:41:11.761593   46632 machine.go:91] provisioned docker machine in 3.843684109s
I1229 14:41:11.761613   46632 client.go:171] LocalClient.Create took 37.27964588s
I1229 14:41:11.761689   46632 start.go:167] duration metric: libmachine.API.Create for "minikube" took 37.28046046s
I1229 14:41:11.762066   46632 start.go:300] post-start starting for "minikube" (driver="docker")
I1229 14:41:11.762110   46632 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1229 14:41:11.762384   46632 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1229 14:41:11.762515   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:11.909470   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:12.020378   46632 ssh_runner.go:195] Run: cat /etc/os-release
I1229 14:41:12.033168   46632 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1229 14:41:12.033231   46632 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1229 14:41:12.033242   46632 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1229 14:41:12.033246   46632 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1229 14:41:12.033262   46632 filesync.go:126] Scanning /Users/hoangphongle/.minikube/addons for local assets ...
I1229 14:41:12.033487   46632 filesync.go:126] Scanning /Users/hoangphongle/.minikube/files for local assets ...
I1229 14:41:12.033544   46632 start.go:303] post-start completed in 271.459323ms
I1229 14:41:12.034656   46632 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1229 14:41:12.173220   46632 profile.go:148] Saving config to /Users/hoangphongle/.minikube/profiles/minikube/config.json ...
I1229 14:41:12.175547   46632 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1229 14:41:12.175807   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:12.359692   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:12.521899   46632 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1229 14:41:12.540224   46632 start.go:128] duration metric: createHost completed in 38.064730288s
I1229 14:41:12.540318   46632 start.go:83] releasing machines lock for "minikube", held for 38.066237431s
I1229 14:41:12.540597   46632 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1229 14:41:12.691194   46632 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1229 14:41:12.691194   46632 ssh_runner.go:195] Run: cat /version.json
I1229 14:41:12.691485   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:12.691691   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:12.937321   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:12.942407   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:13.138853   46632 ssh_runner.go:195] Run: systemctl --version
I1229 14:41:13.808985   46632 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.117715615s)
I1229 14:41:13.809197   46632 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1229 14:41:13.831629   46632 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1229 14:41:13.885479   46632 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1229 14:41:13.885791   46632 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1229 14:41:13.935743   46632 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I1229 14:41:13.935765   46632 start.go:472] detecting cgroup driver to use...
I1229 14:41:13.935801   46632 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1229 14:41:13.936426   46632 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1229 14:41:13.967958   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1229 14:41:13.995644   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1229 14:41:14.020984   46632 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1229 14:41:14.021342   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1229 14:41:14.051854   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1229 14:41:14.071201   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1229 14:41:14.091209   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1229 14:41:14.112967   46632 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1229 14:41:14.133146   46632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1229 14:41:14.151520   46632 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1229 14:41:14.170438   46632 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1229 14:41:14.194433   46632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1229 14:41:14.323945   46632 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1229 14:41:14.470064   46632 start.go:472] detecting cgroup driver to use...
I1229 14:41:14.470149   46632 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1229 14:41:14.470386   46632 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1229 14:41:14.565352   46632 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1229 14:41:14.565636   46632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1229 14:41:14.629627   46632 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1229 14:41:14.675541   46632 ssh_runner.go:195] Run: which cri-dockerd
I1229 14:41:14.686365   46632 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1229 14:41:14.723741   46632 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1229 14:41:14.796879   46632 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1229 14:41:15.033480   46632 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1229 14:41:15.208392   46632 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1229 14:41:15.208870   46632 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1229 14:41:15.258466   46632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1229 14:41:15.403128   46632 ssh_runner.go:195] Run: sudo systemctl restart docker
I1229 14:41:16.052904   46632 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1229 14:41:16.224042   46632 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1229 14:41:16.367116   46632 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1229 14:41:16.527657   46632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1229 14:41:16.679543   46632 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1229 14:41:16.717884   46632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1229 14:41:16.864492   46632 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1229 14:41:17.156860   46632 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1229 14:41:17.157728   46632 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1229 14:41:17.179312   46632 start.go:540] Will wait 60s for crictl version
I1229 14:41:17.179608   46632 ssh_runner.go:195] Run: which crictl
I1229 14:41:17.191963   46632 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1229 14:41:17.405305   46632 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1229 14:41:17.405488   46632 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1229 14:41:17.512451   46632 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1229 14:41:17.568014   46632 out.go:204] ðŸ³  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1229 14:41:17.568312   46632 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1229 14:41:17.905073   46632 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1229 14:41:17.906351   46632 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1229 14:41:17.924878   46632 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1229 14:41:17.953971   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1229 14:41:18.085375   46632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1229 14:41:18.085524   46632 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1229 14:41:18.125816   46632 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1229 14:41:18.126216   46632 docker.go:601] Images already preloaded, skipping extraction
I1229 14:41:18.126436   46632 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1229 14:41:18.183873   46632 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1229 14:41:18.184231   46632 cache_images.go:84] Images are preloaded, skipping loading
I1229 14:41:18.184471   46632 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1229 14:41:18.308363   46632 cni.go:84] Creating CNI manager for ""
I1229 14:41:18.308402   46632 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1229 14:41:18.309077   46632 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1229 14:41:18.309115   46632 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1229 14:41:18.309437   46632 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1229 14:41:18.309653   46632 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1229 14:41:18.309864   46632 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1229 14:41:18.330804   46632 binaries.go:44] Found k8s binaries, skipping transfer
I1229 14:41:18.331061   46632 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1229 14:41:18.355269   46632 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1229 14:41:18.400667   46632 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1229 14:41:18.458728   46632 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1229 14:41:18.500415   46632 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1229 14:41:18.510489   46632 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1229 14:41:18.537602   46632 certs.go:56] Setting up /Users/hoangphongle/.minikube/profiles/minikube for IP: 192.168.49.2
I1229 14:41:18.537642   46632 certs.go:190] acquiring lock for shared ca certs: {Name:mk9bbc3dfc0e9b6f6e909b66cd710404ed55dfd1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:18.537941   46632 certs.go:204] generating minikubeCA CA: /Users/hoangphongle/.minikube/ca.key
I1229 14:41:18.825703   46632 crypto.go:156] Writing cert to /Users/hoangphongle/.minikube/ca.crt ...
I1229 14:41:18.825730   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/ca.crt: {Name:mkf36d785ead2ec20913007ae3324f415115a076 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:18.826190   46632 crypto.go:164] Writing key to /Users/hoangphongle/.minikube/ca.key ...
I1229 14:41:18.826197   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/ca.key: {Name:mka3438ad3b24f40e0a032626541522ebfcc3fb0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:18.826433   46632 certs.go:204] generating proxyClientCA CA: /Users/hoangphongle/.minikube/proxy-client-ca.key
I1229 14:41:18.952544   46632 crypto.go:156] Writing cert to /Users/hoangphongle/.minikube/proxy-client-ca.crt ...
I1229 14:41:18.952555   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/proxy-client-ca.crt: {Name:mk0a93f6316bd30063baadc29ed837c73ec76706 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:18.952910   46632 crypto.go:164] Writing key to /Users/hoangphongle/.minikube/proxy-client-ca.key ...
I1229 14:41:18.952929   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/proxy-client-ca.key: {Name:mkf7ca970e8d47c39845bc5d0150de54de082197 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:18.953858   46632 certs.go:319] generating minikube-user signed cert: /Users/hoangphongle/.minikube/profiles/minikube/client.key
I1229 14:41:18.954220   46632 crypto.go:68] Generating cert /Users/hoangphongle/.minikube/profiles/minikube/client.crt with IP's: []
I1229 14:41:19.260014   46632 crypto.go:156] Writing cert to /Users/hoangphongle/.minikube/profiles/minikube/client.crt ...
I1229 14:41:19.260027   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/client.crt: {Name:mka05a4ddcb336ca113f3c333103a211d6fcc69c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:19.260546   46632 crypto.go:164] Writing key to /Users/hoangphongle/.minikube/profiles/minikube/client.key ...
I1229 14:41:19.260558   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/client.key: {Name:mk2b87fdba9d074c3fbb6f82b9eabdf0c0f8013d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:19.260817   46632 certs.go:319] generating minikube signed cert: /Users/hoangphongle/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1229 14:41:19.260851   46632 crypto.go:68] Generating cert /Users/hoangphongle/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1229 14:41:19.505768   46632 crypto.go:156] Writing cert to /Users/hoangphongle/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1229 14:41:19.505788   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mkf411db8b554e0c64e3425104d2e9088f911e15 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:19.506186   46632 crypto.go:164] Writing key to /Users/hoangphongle/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1229 14:41:19.506194   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk33fb5657b3bd8877c3df6af75257743baf6e70 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:19.506702   46632 certs.go:337] copying /Users/hoangphongle/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/hoangphongle/.minikube/profiles/minikube/apiserver.crt
I1229 14:41:19.507002   46632 certs.go:341] copying /Users/hoangphongle/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/hoangphongle/.minikube/profiles/minikube/apiserver.key
I1229 14:41:19.507225   46632 certs.go:319] generating aggregator signed cert: /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.key
I1229 14:41:19.507308   46632 crypto.go:68] Generating cert /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1229 14:41:19.573949   46632 crypto.go:156] Writing cert to /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.crt ...
I1229 14:41:19.573967   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.crt: {Name:mk89c64e0e1616e4f2aed05d4ff0cd18d9e54cb3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:19.574530   46632 crypto.go:164] Writing key to /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.key ...
I1229 14:41:19.574553   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.key: {Name:mk1ddf048b564833c5e77a71a0249fce21d76283 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:19.575556   46632 certs.go:437] found cert: /Users/hoangphongle/.minikube/certs/Users/hoangphongle/.minikube/certs/ca-key.pem (1675 bytes)
I1229 14:41:19.575703   46632 certs.go:437] found cert: /Users/hoangphongle/.minikube/certs/Users/hoangphongle/.minikube/certs/ca.pem (1094 bytes)
I1229 14:41:19.575837   46632 certs.go:437] found cert: /Users/hoangphongle/.minikube/certs/Users/hoangphongle/.minikube/certs/cert.pem (1139 bytes)
I1229 14:41:19.575932   46632 certs.go:437] found cert: /Users/hoangphongle/.minikube/certs/Users/hoangphongle/.minikube/certs/key.pem (1679 bytes)
I1229 14:41:19.577719   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1229 14:41:19.623009   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1229 14:41:19.665563   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1229 14:41:19.726453   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1229 14:41:19.777755   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1229 14:41:19.827804   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1229 14:41:19.881231   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1229 14:41:19.935591   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1229 14:41:20.020496   46632 ssh_runner.go:362] scp /Users/hoangphongle/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1229 14:41:20.070493   46632 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1229 14:41:20.116899   46632 ssh_runner.go:195] Run: openssl version
I1229 14:41:20.131742   46632 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1229 14:41:20.150483   46632 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1229 14:41:20.159942   46632 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Dec 29 07:41 /usr/share/ca-certificates/minikubeCA.pem
I1229 14:41:20.160074   46632 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1229 14:41:20.186868   46632 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1229 14:41:20.211046   46632 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1229 14:41:20.220910   46632 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1229 14:41:20.221003   46632 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1229 14:41:20.221318   46632 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1229 14:41:20.255127   46632 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1229 14:41:20.282144   46632 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1229 14:41:20.312395   46632 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I1229 14:41:20.312559   46632 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1229 14:41:20.329581   46632 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1229 14:41:20.329632   46632 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1229 14:41:20.533623   46632 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I1229 14:41:20.536677   46632 kubeadm.go:322] [preflight] Running pre-flight checks
I1229 14:41:21.243097   46632 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I1229 14:41:21.243337   46632 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1229 14:41:21.243547   46632 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1229 14:41:21.920615   46632 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1229 14:41:21.924687   46632 out.go:204]     â–ª Generating certificates and keys ...
I1229 14:41:21.924858   46632 kubeadm.go:322] [certs] Using existing ca certificate authority
I1229 14:41:21.928522   46632 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I1229 14:41:22.182433   46632 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I1229 14:41:22.314781   46632 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I1229 14:41:22.585112   46632 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I1229 14:41:22.813542   46632 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I1229 14:41:23.024946   46632 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I1229 14:41:23.029835   46632 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1229 14:41:23.100693   46632 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I1229 14:41:23.101983   46632 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1229 14:41:23.446327   46632 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I1229 14:41:23.533428   46632 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I1229 14:41:23.909411   46632 kubeadm.go:322] [certs] Generating "sa" key and public key
I1229 14:41:23.910339   46632 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1229 14:41:24.100416   46632 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I1229 14:41:24.366261   46632 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1229 14:41:25.182490   46632 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1229 14:41:25.662311   46632 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1229 14:41:25.671710   46632 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1229 14:41:25.678376   46632 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1229 14:41:25.681155   46632 out.go:204]     â–ª Booting up control plane ...
I1229 14:41:25.681384   46632 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1229 14:41:25.681592   46632 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1229 14:41:25.684895   46632 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1229 14:41:25.722825   46632 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1229 14:41:25.723012   46632 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1229 14:41:25.723104   46632 kubeadm.go:322] [kubelet-start] Starting the kubelet
I1229 14:41:26.141983   46632 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1229 14:41:36.642660   46632 kubeadm.go:322] [apiclient] All control plane components are healthy after 10.507830 seconds
I1229 14:41:36.642906   46632 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1229 14:41:36.677584   46632 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1229 14:41:37.241205   46632 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I1229 14:41:37.241908   46632 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1229 14:41:37.758336   46632 kubeadm.go:322] [bootstrap-token] Using token: 8xqn2q.z9widz9gwy49d5fa
I1229 14:41:37.760329   46632 out.go:204]     â–ª Configuring RBAC rules ...
I1229 14:41:37.760619   46632 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1229 14:41:37.790983   46632 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1229 14:41:37.820116   46632 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1229 14:41:37.831156   46632 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1229 14:41:37.841911   46632 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1229 14:41:37.852669   46632 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1229 14:41:37.885681   46632 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1229 14:41:38.282085   46632 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I1229 14:41:38.335941   46632 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I1229 14:41:38.338968   46632 kubeadm.go:322] 
I1229 14:41:38.339120   46632 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I1229 14:41:38.339130   46632 kubeadm.go:322] 
I1229 14:41:38.339256   46632 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I1229 14:41:38.339275   46632 kubeadm.go:322] 
I1229 14:41:38.339327   46632 kubeadm.go:322]   mkdir -p $HOME/.kube
I1229 14:41:38.339814   46632 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1229 14:41:38.339920   46632 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1229 14:41:38.339938   46632 kubeadm.go:322] 
I1229 14:41:38.340047   46632 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I1229 14:41:38.340054   46632 kubeadm.go:322] 
I1229 14:41:38.340158   46632 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1229 14:41:38.340175   46632 kubeadm.go:322] 
I1229 14:41:38.340316   46632 kubeadm.go:322] You should now deploy a pod network to the cluster.
I1229 14:41:38.340487   46632 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1229 14:41:38.340629   46632 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1229 14:41:38.340659   46632 kubeadm.go:322] 
I1229 14:41:38.343640   46632 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I1229 14:41:38.343828   46632 kubeadm.go:322] and service account keys on each node and then running the following as root:
I1229 14:41:38.343837   46632 kubeadm.go:322] 
I1229 14:41:38.344046   46632 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token 8xqn2q.z9widz9gwy49d5fa \
I1229 14:41:38.344257   46632 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:49ff57fcdfb14775cfe9e65f894d28a5d73f37e2ca23b79b95bc71313057fe4c \
I1229 14:41:38.344306   46632 kubeadm.go:322] 	--control-plane 
I1229 14:41:38.344313   46632 kubeadm.go:322] 
I1229 14:41:38.344498   46632 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I1229 14:41:38.344505   46632 kubeadm.go:322] 
I1229 14:41:38.344670   46632 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token 8xqn2q.z9widz9gwy49d5fa \
I1229 14:41:38.344924   46632 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:49ff57fcdfb14775cfe9e65f894d28a5d73f37e2ca23b79b95bc71313057fe4c 
I1229 14:41:38.355378   46632 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I1229 14:41:38.355661   46632 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1229 14:41:38.355682   46632 cni.go:84] Creating CNI manager for ""
I1229 14:41:38.355709   46632 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1229 14:41:38.359749   46632 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I1229 14:41:38.362316   46632 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1229 14:41:38.424030   46632 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1229 14:41:38.546255   46632 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1229 14:41:38.546677   46632 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1229 14:41:38.546734   46632 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_12_29T14_41_38_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1229 14:41:38.630022   46632 ops.go:34] apiserver oom_adj: -16
I1229 14:41:39.495479   46632 kubeadm.go:1081] duration metric: took 948.970779ms to wait for elevateKubeSystemPrivileges.
I1229 14:41:39.495514   46632 kubeadm.go:406] StartCluster complete in 19.273937578s
I1229 14:41:39.495731   46632 settings.go:142] acquiring lock: {Name:mk625d667d9a0c5827e106ab7fa5b5c45d2bf330 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:39.495946   46632 settings.go:150] Updating kubeconfig:  /Users/hoangphongle/.kube/config
I1229 14:41:39.498181   46632 lock.go:35] WriteFile acquiring /Users/hoangphongle/.kube/config: {Name:mkdfca4f959fec33ec4ad894c7c25cbdc37ba837 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1229 14:41:39.499355   46632 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1229 14:41:39.499419   46632 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1229 14:41:39.499623   46632 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1229 14:41:39.499889   46632 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1229 14:41:39.499890   46632 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1229 14:41:39.499922   46632 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1229 14:41:39.499925   46632 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1229 14:41:39.500727   46632 host.go:66] Checking if "minikube" exists ...
I1229 14:41:39.502550   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1229 14:41:39.502644   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1229 14:41:39.608045   46632 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1229 14:41:39.608271   46632 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1229 14:41:39.610128   46632 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I1229 14:41:39.614501   46632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1229 14:41:39.827148   46632 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1229 14:41:39.827191   46632 host.go:66] Checking if "minikube" exists ...
I1229 14:41:39.827953   46632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1229 14:41:39.835245   46632 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1229 14:41:39.837289   46632 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1229 14:41:39.837305   46632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1229 14:41:39.837529   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:40.026606   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:40.035427   46632 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1229 14:41:40.035459   46632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1229 14:41:40.035704   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1229 14:41:40.186307   46632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54619 SSHKeyPath:/Users/hoangphongle/.minikube/machines/minikube/id_rsa Username:docker}
I1229 14:41:40.242127   46632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1229 14:41:40.242143   46632 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1229 14:41:40.426104   46632 api_server.go:52] waiting for apiserver process to appear ...
I1229 14:41:40.426329   46632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1229 14:41:40.550693   46632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1229 14:41:40.643956   46632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1229 14:41:42.491811   46632 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (2.249524689s)
I1229 14:41:42.491890   46632 start.go:926] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS's ConfigMap
I1229 14:41:42.491900   46632 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.065480928s)
I1229 14:41:42.491918   46632 api_server.go:72] duration metric: took 2.883523501s to wait for apiserver process to appear ...
I1229 14:41:42.491926   46632 api_server.go:88] waiting for apiserver healthz status ...
I1229 14:41:42.491960   46632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54623/healthz ...
I1229 14:41:42.510767   46632 api_server.go:279] https://127.0.0.1:54623/healthz returned 200:
ok
I1229 14:41:42.514040   46632 api_server.go:141] control plane version: v1.28.3
I1229 14:41:42.514067   46632 api_server.go:131] duration metric: took 22.127239ms to wait for apiserver health ...
I1229 14:41:42.514472   46632 system_pods.go:43] waiting for kube-system pods to appear ...
I1229 14:41:42.552558   46632 system_pods.go:59] 4 kube-system pods found
I1229 14:41:42.552585   46632 system_pods.go:61] "etcd-minikube" [9d4c0fa6-2575-4ea7-b5dd-728d3ef685ac] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1229 14:41:42.552599   46632 system_pods.go:61] "kube-apiserver-minikube" [1628b9da-6a5a-4af6-8029-0485a431591c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1229 14:41:42.552604   46632 system_pods.go:61] "kube-controller-manager-minikube" [65d3844b-7560-47d0-937b-cc26b0ca044c] Running
I1229 14:41:42.552609   46632 system_pods.go:61] "kube-scheduler-minikube" [74699dce-cba3-41cf-b7bc-24a1efeb36e8] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1229 14:41:42.552615   46632 system_pods.go:74] duration metric: took 38.130296ms to wait for pod list to return data ...
I1229 14:41:42.552627   46632 kubeadm.go:581] duration metric: took 2.944228632s to wait for : map[apiserver:true system_pods:true] ...
I1229 14:41:42.552637   46632 node_conditions.go:102] verifying NodePressure condition ...
I1229 14:41:42.567221   46632 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1229 14:41:42.567243   46632 node_conditions.go:123] node cpu capacity is 2
I1229 14:41:42.567266   46632 node_conditions.go:105] duration metric: took 14.625256ms to run NodePressure ...
I1229 14:41:42.567276   46632 start.go:228] waiting for startup goroutines ...
I1229 14:41:42.834455   46632 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.283658529s)
I1229 14:41:42.834517   46632 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.190471939s)
I1229 14:41:42.852457   46632 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I1229 14:41:42.856634   46632 addons.go:502] enable addons completed in 3.357417655s: enabled=[storage-provisioner default-storageclass]
I1229 14:41:42.856666   46632 start.go:233] waiting for cluster config update ...
I1229 14:41:42.856684   46632 start.go:242] writing updated cluster config ...
I1229 14:41:42.866489   46632 ssh_runner.go:195] Run: rm -f paused
I1229 14:41:42.949778   46632 start.go:600] kubectl: 1.24.1, cluster: 1.28.3 (minor skew: 4)
I1229 14:41:42.950973   46632 out.go:177] 
W1229 14:41:42.953270   46632 out.go:239] â—  /usr/local/bin/kubectl is version 1.24.1, which may have incompatibilities with Kubernetes 1.28.3.
I1229 14:41:42.956785   46632 out.go:177]     â–ª Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I1229 14:41:42.959705   46632 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Dec 29 07:41:15 minikube dockerd[995]: time="2023-12-29T07:41:15.880758466Z" level=info msg="Loading containers: done."
Dec 29 07:41:15 minikube dockerd[995]: time="2023-12-29T07:41:15.915473926Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Dec 29 07:41:15 minikube dockerd[995]: time="2023-12-29T07:41:15.915702741Z" level=info msg="Daemon has completed initialization"
Dec 29 07:41:16 minikube dockerd[995]: time="2023-12-29T07:41:16.066400585Z" level=info msg="API listen on /var/run/docker.sock"
Dec 29 07:41:16 minikube dockerd[995]: time="2023-12-29T07:41:16.066843955Z" level=info msg="API listen on [::]:2376"
Dec 29 07:41:16 minikube systemd[1]: Started Docker Application Container Engine.
Dec 29 07:41:16 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Start docker client with request timeout 0s"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Loaded network plugin cni"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Docker Info: &{ID:c9f8610c-288d-49a7-956a-62f1c512ed1b Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:36 SystemTime:2023-12-29T07:41:17.147211093Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:2 NEventsListener:0 KernelVersion:5.10.104-linuxkit OperatingSystem:Ubuntu 22.04.3 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00020d180 NCPU:2 MemTotal:4125892608 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 29 07:41:17 minikube cri-dockerd[1212]: time="2023-12-29T07:41:17Z" level=info msg="Start cri-dockerd grpc backend"
Dec 29 07:41:17 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 29 07:41:28 minikube cri-dockerd[1212]: time="2023-12-29T07:41:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b86d52af9b22545743be742f27a48be89b2405cfc08e8cb58e8d0e2d956b59b7/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:28 minikube cri-dockerd[1212]: time="2023-12-29T07:41:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/212867a0b467f9e6ea1e9914299ead5645c48561a54ff9f44324d5d526b757d7/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:28 minikube cri-dockerd[1212]: time="2023-12-29T07:41:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/52bde2274e55ef2ae7c8a4ce569c9a341959deef97c2423a9b7f052293bc3e24/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:28 minikube cri-dockerd[1212]: time="2023-12-29T07:41:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2fb5efa861e74b506d060262d0765865e0e25fe64119e05c2bb364ef366b2db3/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:51 minikube cri-dockerd[1212]: time="2023-12-29T07:41:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c66b53b86c8e64f54320802197482906842576cc4de610af801ed6657c166f91/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:51 minikube cri-dockerd[1212]: time="2023-12-29T07:41:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea683c78022a521781622d2f16a11dd74aaa60c5f214f9853b12dbead3b69071/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:52 minikube dockerd[995]: time="2023-12-29T07:41:52.082969132Z" level=info msg="ignoring event" container=39348c97d7986833cddbbbf7591cf635cca098252654b4fc649052b6e73b23ba module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 29 07:41:53 minikube cri-dockerd[1212]: time="2023-12-29T07:41:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c3e994e6a9e9340548c52861fbc47649a76b378f89886aca78b56a7c5f8ae20c/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 29 07:41:59 minikube cri-dockerd[1212]: time="2023-12-29T07:41:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 29 07:55:46 minikube cri-dockerd[1212]: time="2023-12-29T07:55:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/31cfcfc46e30108e65128016a0951d2d603fbb66bceea2af644ec3bbb75db149/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 29 07:55:46 minikube cri-dockerd[1212]: time="2023-12-29T07:55:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c4f31dfc382092382a0f6047fc0db3ed002b9281e8ec30766d13e0b9574f92c5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 29 07:55:46 minikube cri-dockerd[1212]: time="2023-12-29T07:55:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b6a5a0569d4997ebb98d513cc995d6e9cd4486dcad5ab3816ef0afff25bf2913/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 29 07:55:52 minikube dockerd[995]: time="2023-12-29T07:55:52.701203038Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:55:52 minikube dockerd[995]: time="2023-12-29T07:55:52.702057676Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:55:59 minikube dockerd[995]: time="2023-12-29T07:55:59.291876365Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:55:59 minikube dockerd[995]: time="2023-12-29T07:55:59.292738100Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:56:03 minikube dockerd[995]: time="2023-12-29T07:56:03.643067326Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:56:03 minikube dockerd[995]: time="2023-12-29T07:56:03.644352152Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:56:06 minikube dockerd[995]: time="2023-12-29T07:56:06.978860453Z" level=warning msg="reference for unknown type: application/vnd.docker.distribution.manifest.v1+prettyjws" digest="sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451" remote="docker.io/library/nginx:1.7.9"
Dec 29 07:56:08 minikube dockerd[995]: time="2023-12-29T07:56:08.169021903Z" level=warning msg="Error persisting manifest" digest="sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451" error="error committing manifest to content store: commit failed: unexpected commit digest sha256:f7c4a2c102b40b990e80a2578e845054c477fd941871a2ba5407a51a087b7476, expected sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451: failed precondition" remote="docker.io/library/nginx:1.7.9"
Dec 29 07:56:08 minikube dockerd[995]: time="2023-12-29T07:56:08.170360047Z" level=warning msg="[DEPRECATION NOTICE] Docker Image Format v1, and Docker Image manifest version 2, schema 1 support will be removed in an upcoming release. Suggest the author of docker.io/library/nginx:1.7.9 to upgrade the image to the OCI Format, or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/"
Dec 29 07:56:18 minikube cri-dockerd[1212]: time="2023-12-29T07:56:18Z" level=info msg="Pulling image nginx:1.7.9: 6f5424ebd796: Extracting [=========================>                         ]  18.87MB/37.22MB"
Dec 29 07:56:28 minikube cri-dockerd[1212]: time="2023-12-29T07:56:28Z" level=info msg="Pulling image nginx:1.7.9: 6f5424ebd796: Extracting [================================================>  ]  36.18MB/37.22MB"
Dec 29 07:56:30 minikube cri-dockerd[1212]: time="2023-12-29T07:56:30Z" level=info msg="Stop pulling image nginx:1.7.9: Status: Downloaded newer image for nginx:1.7.9"
Dec 29 07:56:34 minikube dockerd[995]: time="2023-12-29T07:56:34.265008980Z" level=warning msg="reference for unknown type: application/vnd.docker.distribution.manifest.v1+prettyjws" digest="sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451" remote="docker.io/library/nginx:1.7.9"
Dec 29 07:56:35 minikube dockerd[995]: time="2023-12-29T07:56:35.484569278Z" level=warning msg="Error persisting manifest" digest="sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451" error="error committing manifest to content store: commit failed: unexpected commit digest sha256:9d1767c148f1441034c1fcb8e15b3f9a2488fc72965cbb37bd6262fc3e8737fb, expected sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451: failed precondition" remote="docker.io/library/nginx:1.7.9"
Dec 29 07:56:35 minikube dockerd[995]: time="2023-12-29T07:56:35.484707697Z" level=warning msg="[DEPRECATION NOTICE] Docker Image Format v1, and Docker Image manifest version 2, schema 1 support will be removed in an upcoming release. Suggest the author of docker.io/library/nginx:1.7.9 to upgrade the image to the OCI Format, or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/"
Dec 29 07:56:35 minikube cri-dockerd[1212]: time="2023-12-29T07:56:35Z" level=info msg="Stop pulling image nginx:1.7.9: Status: Image is up to date for nginx:1.7.9"
Dec 29 07:56:38 minikube dockerd[995]: time="2023-12-29T07:56:38.530833721Z" level=warning msg="reference for unknown type: application/vnd.docker.distribution.manifest.v1+prettyjws" digest="sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451" remote="docker.io/library/nginx:1.7.9"
Dec 29 07:56:40 minikube dockerd[995]: time="2023-12-29T07:56:40.232599784Z" level=warning msg="Error persisting manifest" digest="sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451" error="error committing manifest to content store: commit failed: unexpected commit digest sha256:2c1cf61db42d29ad9deadcca1e9abdac4ef1d4d0ab5b0b5720c7681155f1a68b, expected sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451: failed precondition" remote="docker.io/library/nginx:1.7.9"
Dec 29 07:56:40 minikube dockerd[995]: time="2023-12-29T07:56:40.235170034Z" level=warning msg="[DEPRECATION NOTICE] Docker Image Format v1, and Docker Image manifest version 2, schema 1 support will be removed in an upcoming release. Suggest the author of docker.io/library/nginx:1.7.9 to upgrade the image to the OCI Format, or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/"
Dec 29 07:56:40 minikube cri-dockerd[1212]: time="2023-12-29T07:56:40Z" level=info msg="Stop pulling image nginx:1.7.9: Status: Image is up to date for nginx:1.7.9"
Dec 29 07:56:45 minikube dockerd[995]: time="2023-12-29T07:56:45.229817405Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:56:45 minikube dockerd[995]: time="2023-12-29T07:56:45.230317651Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:56:50 minikube dockerd[995]: time="2023-12-29T07:56:50.250734974Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:56:50 minikube dockerd[995]: time="2023-12-29T07:56:50.250814960Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:56:54 minikube dockerd[995]: time="2023-12-29T07:56:54.631931458Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:56:54 minikube dockerd[995]: time="2023-12-29T07:56:54.632001938Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:57:16 minikube dockerd[995]: time="2023-12-29T07:57:16.564840595Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:57:16 minikube dockerd[995]: time="2023-12-29T07:57:16.565705830Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 29 07:57:21 minikube dockerd[995]: time="2023-12-29T07:57:21.808670293Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 29 07:57:21 minikube dockerd[995]: time="2023-12-29T07:57:21.808754372Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ca07ffd032512       nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451   46 seconds ago      Running             nginx                     0                   b6a5a0569d499       phpfpm-nginx-57dc94b95f-54kf4
91648c47c17e1       nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451   51 seconds ago      Running             nginx                     0                   31cfcfc46e301       phpfpm-nginx-57dc94b95f-csmqh
a4773cd5e62c0       nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451   56 seconds ago      Running             nginx                     0                   c4f31dfc38209       phpfpm-nginx-57dc94b95f-bbq62
10d51ab518d29       ead0a4a53df89                                                                   15 minutes ago      Running             coredns                   0                   c3e994e6a9e93       coredns-5dd5756b68-d7qq5
2cac54d656beb       6e38f40d628db                                                                   15 minutes ago      Running             storage-provisioner       1                   c66b53b86c8e6       storage-provisioner
6c77293bdaf4c       bfc896cf80fba                                                                   15 minutes ago      Running             kube-proxy                0                   ea683c78022a5       kube-proxy-dbf9l
39348c97d7986       6e38f40d628db                                                                   15 minutes ago      Exited              storage-provisioner       0                   c66b53b86c8e6       storage-provisioner
2a586cba8ba47       5374347291230                                                                   15 minutes ago      Running             kube-apiserver            0                   2fb5efa861e74       kube-apiserver-minikube
03d3cfa9ebebb       10baa1ca17068                                                                   15 minutes ago      Running             kube-controller-manager   0                   52bde2274e55e       kube-controller-manager-minikube
907df3897baaa       73deb9a3f7025                                                                   15 minutes ago      Running             etcd                      0                   212867a0b467f       etcd-minikube
5ecedb5a7b660       6d1b4fd1b182d                                                                   15 minutes ago      Running             kube-scheduler            0                   b86d52af9b225       kube-scheduler-minikube

* 
* ==> coredns [10d51ab518d2] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:35676 - 49747 "HINFO IN 78141853168567804.3513460018092053327. udp 55 false 512" NOERROR qr,rd,ra 55 0.319194735s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_12_29T14_41_38_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 29 Dec 2023 07:41:34 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 29 Dec 2023 07:57:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 29 Dec 2023 07:56:47 +0000   Fri, 29 Dec 2023 07:41:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 29 Dec 2023 07:56:47 +0000   Fri, 29 Dec 2023 07:41:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 29 Dec 2023 07:56:47 +0000   Fri, 29 Dec 2023 07:41:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 29 Dec 2023 07:56:47 +0000   Fri, 29 Dec 2023 07:41:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029192Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029192Ki
  pods:               110
System Info:
  Machine ID:                 7e94cc14f0d342f2a4f2c270b1296418
  System UUID:                4f8a7624-8e8a-468c-b15a-9972e72188e3
  Boot ID:                    1d9433de-61dc-4740-a389-704f75f54b54
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     phpfpm-nginx-57dc94b95f-54kf4       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         109s
  default                     phpfpm-nginx-57dc94b95f-bbq62       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         109s
  default                     phpfpm-nginx-57dc94b95f-csmqh       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         109s
  kube-system                 coredns-5dd5756b68-d7qq5            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     15m
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         15m
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  kube-system                 kube-proxy-dbf9l                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 15m                kube-proxy       
  Normal  NodeHasSufficientMemory  16m (x8 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    16m (x8 over 16m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     16m (x7 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  16m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  15m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    15m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     15m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 15m                kubelet          Starting kubelet.
  Normal  NodeNotReady             15m                kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeAllocatableEnforced  15m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                15m                kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           15m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Dec29 01:51] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20200925/tbprint-173)
[  +0.000000] ACPI: setting ELCR to 0200 (from 06e0)
[  +0.000000] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.012004] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.034831] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.002562] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +2.207804] hrtimer: interrupt took 151265678 ns
[  +8.930778] grpcfuse: loading out-of-tree module taints kernel.

* 
* ==> etcd [907df3897baa] <==
* {"level":"info","ts":"2023-12-29T07:41:29.084126Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-12-29T07:41:29.092722Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-12-29T07:41:29.100241Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-29T07:41:29.100307Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-29T07:41:29.100549Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-12-29T07:41:29.100669Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-12-29T07:41:29.100731Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-29T07:41:29.100783Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-29T07:41:29.100803Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-29T07:41:29.102945Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-12-29T07:41:29.102996Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-12-29T07:41:29.545315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2023-12-29T07:41:29.54566Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2023-12-29T07:41:29.545801Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2023-12-29T07:41:29.54604Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2023-12-29T07:41:29.54619Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2023-12-29T07:41:29.547236Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2023-12-29T07:41:29.548016Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2023-12-29T07:41:29.550338Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-29T07:41:29.552289Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-12-29T07:41:29.552352Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-29T07:41:29.558073Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-12-29T07:41:29.560213Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-29T07:41:29.565188Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-12-29T07:41:29.565281Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-12-29T07:41:29.56817Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-29T07:41:29.568987Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-29T07:41:29.569337Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-29T07:41:29.594679Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-12-29T07:42:16.24892Z","caller":"traceutil/trace.go:171","msg":"trace[1022862613] transaction","detail":"{read_only:false; response_revision:430; number_of_response:1; }","duration":"240.431638ms","start":"2023-12-29T07:42:16.00846Z","end":"2023-12-29T07:42:16.248891Z","steps":["trace[1022862613] 'process raft request'  (duration: 240.21647ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-29T07:50:36.902854Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-29T07:50:36.560257Z","time spent":"342.55154ms","remote":"127.0.0.1:34224","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":2,"response size":1932,"request content":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" "}
{"level":"warn","ts":"2023-12-29T07:50:38.428676Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.397422ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2023-12-29T07:50:38.428785Z","caller":"traceutil/trace.go:171","msg":"trace[1549727680] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:825; }","duration":"146.474586ms","start":"2023-12-29T07:50:38.282262Z","end":"2023-12-29T07:50:38.428736Z","steps":["trace[1549727680] 'agreement among raft nodes before linearized reading'  (duration: 90.055511ms)","trace[1549727680] 'range keys from in-memory index tree'  (duration: 56.31118ms)"],"step_count":2}
{"level":"info","ts":"2023-12-29T07:51:01.658831Z","caller":"traceutil/trace.go:171","msg":"trace[530749370] transaction","detail":"{read_only:false; response_revision:844; number_of_response:1; }","duration":"952.385669ms","start":"2023-12-29T07:51:00.706392Z","end":"2023-12-29T07:51:01.658777Z","steps":["trace[530749370] 'process raft request'  (duration: 952.262145ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-29T07:51:01.678802Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-29T07:51:00.706377Z","time spent":"952.528178ms","remote":"127.0.0.1:34216","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:841 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-12-29T07:51:30.9758Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":633}
{"level":"info","ts":"2023-12-29T07:51:31.102371Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":633,"took":"125.79396ms","hash":917490962}
{"level":"info","ts":"2023-12-29T07:51:31.102444Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":917490962,"revision":633,"compact-revision":-1}
{"level":"info","ts":"2023-12-29T07:53:12.711211Z","caller":"traceutil/trace.go:171","msg":"trace[1615534659] transaction","detail":"{read_only:false; response_revision:945; number_of_response:1; }","duration":"620.057097ms","start":"2023-12-29T07:53:12.091092Z","end":"2023-12-29T07:53:12.711149Z","steps":["trace[1615534659] 'process raft request'  (duration: 619.891326ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-29T07:53:12.71143Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-29T07:53:12.091049Z","time spent":"620.28113ms","remote":"127.0.0.1:34240","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:938 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2023-12-29T07:54:43.226111Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"179.132941ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2023-12-29T07:54:43.360684Z","caller":"traceutil/trace.go:171","msg":"trace[223002683] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1013; }","duration":"313.710852ms","start":"2023-12-29T07:54:43.046946Z","end":"2023-12-29T07:54:43.360657Z","steps":["trace[223002683] 'agreement among raft nodes before linearized reading'  (duration: 156.922065ms)","trace[223002683] 'range keys from bolt db'  (duration: 22.149858ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-29T07:54:43.360759Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-29T07:54:43.005378Z","time spent":"355.352468ms","remote":"127.0.0.1:34216","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":625,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2023-12-29T07:54:43.469624Z","caller":"traceutil/trace.go:171","msg":"trace[298865273] linearizableReadLoop","detail":"{readStateIndex:1181; appliedIndex:1180; }","duration":"108.386312ms","start":"2023-12-29T07:54:43.361214Z","end":"2023-12-29T07:54:43.469601Z","steps":["trace[298865273] 'read index received'  (duration: 108.057671ms)","trace[298865273] 'applied index is now lower than readState.Index'  (duration: 326.694Âµs)"],"step_count":2}
{"level":"warn","ts":"2023-12-29T07:54:43.471008Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.78724ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-12-29T07:54:43.471078Z","caller":"traceutil/trace.go:171","msg":"trace[70120388] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:1013; }","duration":"109.875405ms","start":"2023-12-29T07:54:43.361186Z","end":"2023-12-29T07:54:43.471061Z","steps":["trace[70120388] 'agreement among raft nodes before linearized reading'  (duration: 109.565199ms)"],"step_count":1}
{"level":"info","ts":"2023-12-29T07:54:54.692944Z","caller":"traceutil/trace.go:171","msg":"trace[319799031] linearizableReadLoop","detail":"{readStateIndex:1191; appliedIndex:1191; }","duration":"199.108679ms","start":"2023-12-29T07:54:54.493817Z","end":"2023-12-29T07:54:54.692926Z","steps":["trace[319799031] 'read index received'  (duration: 199.101718ms)","trace[319799031] 'applied index is now lower than readState.Index'  (duration: 5.8Âµs)"],"step_count":2}
{"level":"warn","ts":"2023-12-29T07:54:54.693094Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.293787ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2023-12-29T07:54:54.693128Z","caller":"traceutil/trace.go:171","msg":"trace[322649763] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1021; }","duration":"199.340899ms","start":"2023-12-29T07:54:54.493777Z","end":"2023-12-29T07:54:54.693118Z","steps":["trace[322649763] 'agreement among raft nodes before linearized reading'  (duration: 199.250194ms)"],"step_count":1}
{"level":"info","ts":"2023-12-29T07:55:02.436299Z","caller":"traceutil/trace.go:171","msg":"trace[1704097962] transaction","detail":"{read_only:false; response_revision:1028; number_of_response:1; }","duration":"128.339557ms","start":"2023-12-29T07:55:02.307443Z","end":"2023-12-29T07:55:02.435782Z","steps":["trace[1704097962] 'process raft request'  (duration: 32.744183ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-29T07:55:44.859439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"128.188945ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-12-29T07:55:44.859494Z","caller":"traceutil/trace.go:171","msg":"trace[1370965892] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1090; }","duration":"128.252098ms","start":"2023-12-29T07:55:44.731229Z","end":"2023-12-29T07:55:44.859481Z","steps":["trace[1370965892] 'get authentication metadata'  (duration: 60.572735ms)","trace[1370965892] 'range keys from in-memory index tree'  (duration: 67.610695ms)"],"step_count":2}
{"level":"info","ts":"2023-12-29T07:55:53.698617Z","caller":"traceutil/trace.go:171","msg":"trace[840958933] transaction","detail":"{read_only:false; response_revision:1105; number_of_response:1; }","duration":"907.390825ms","start":"2023-12-29T07:55:52.791205Z","end":"2023-12-29T07:55:53.698596Z","steps":["trace[840958933] 'process raft request'  (duration: 905.869824ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-29T07:55:53.698805Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-29T07:55:52.791069Z","time spent":"907.652839ms","remote":"127.0.0.1:34194","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":909,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/phpfpm-nginx-57dc94b95f-bbq62.17a53e51425e2a55\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/phpfpm-nginx-57dc94b95f-bbq62.17a53e51425e2a55\" value_size:820 lease:8128026134456591296 >> failure:<>"}
{"level":"info","ts":"2023-12-29T07:55:56.368715Z","caller":"traceutil/trace.go:171","msg":"trace[1757211272] transaction","detail":"{read_only:false; response_revision:1110; number_of_response:1; }","duration":"122.086689ms","start":"2023-12-29T07:55:56.246576Z","end":"2023-12-29T07:55:56.368663Z","steps":["trace[1757211272] 'process raft request'  (duration: 121.937054ms)"],"step_count":1}
{"level":"info","ts":"2023-12-29T07:56:14.116027Z","caller":"traceutil/trace.go:171","msg":"trace[1391647499] transaction","detail":"{read_only:false; response_revision:1130; number_of_response:1; }","duration":"116.884074ms","start":"2023-12-29T07:56:13.999075Z","end":"2023-12-29T07:56:14.115959Z","steps":["trace[1391647499] 'process raft request'  (duration: 100.387164ms)"],"step_count":1}
{"level":"info","ts":"2023-12-29T07:56:30.856702Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":867}
{"level":"info","ts":"2023-12-29T07:56:30.871373Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":867,"took":"14.161995ms","hash":4072079471}
{"level":"info","ts":"2023-12-29T07:56:30.871502Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4072079471,"revision":867,"compact-revision":633}
{"level":"info","ts":"2023-12-29T07:57:22.45477Z","caller":"traceutil/trace.go:171","msg":"trace[1706838514] transaction","detail":"{read_only:false; response_revision:1233; number_of_response:1; }","duration":"104.753633ms","start":"2023-12-29T07:57:22.349925Z","end":"2023-12-29T07:57:22.454679Z","steps":["trace[1706838514] 'process raft request'  (duration: 104.179815ms)"],"step_count":1}

* 
* ==> kernel <==
*  07:57:27 up  6:05,  0 users,  load average: 8.46, 5.27, 3.20
Linux minikube 5.10.104-linuxkit #1 SMP Thu Mar 17 17:08:06 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [2a586cba8ba4] <==
* I1229 07:42:16.010610       1 trace.go:236] Trace[249354795]: "Update" accept:application/json, */*,audit-id:13512db3-c577-410a-8e8f-16d7f725b74e,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (29-Dec-2023 07:42:15.510) (total time: 500ms):
Trace[249354795]: [500.159193ms] [500.159193ms] END
I1229 07:48:54.893409       1 trace.go:236] Trace[1804136474]: "Get" accept:application/json, */*,audit-id:807b5e7d-28c1-4789-838d-e19d0e2e64e2,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (29-Dec-2023 07:48:53.961) (total time: 958ms):
Trace[1804136474]: ---"About to write a response" 958ms (07:48:54.889)
Trace[1804136474]: [958.615451ms] [958.615451ms] END
I1229 07:48:55.144792       1 trace.go:236] Trace[611850752]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a511b3b7-5691-4918-b427-575be6cafb6a,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (29-Dec-2023 07:48:53.943) (total time: 1231ms):
Trace[611850752]: ["GuaranteedUpdate etcd3" audit-id:a511b3b7-5691-4918-b427-575be6cafb6a,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1230ms (07:48:53.944)
Trace[611850752]:  ---"About to Encode" 796ms (07:48:54.710)
Trace[611850752]:  ---"Txn call completed" 433ms (07:48:55.144)]
Trace[611850752]: [1.231611803s] [1.231611803s] END
I1229 07:50:37.440463       1 trace.go:236] Trace[1215258736]: "Get" accept:application/json, */*,audit-id:66388e3e-1b18-4a31-95a4-c6a54f293a04,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (29-Dec-2023 07:50:36.900) (total time: 540ms):
Trace[1215258736]: ---"About to write a response" 540ms (07:50:37.440)
Trace[1215258736]: [540.245279ms] [540.245279ms] END
I1229 07:50:37.501962       1 trace.go:236] Trace[910390813]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:180aebb0-2dae-409d-a091-a4fb40f507d0,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:LIST (29-Dec-2023 07:50:35.346) (total time: 2155ms):
Trace[910390813]: ["List(recursive=true) etcd3" audit-id:180aebb0-2dae-409d-a091-a4fb40f507d0,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 2155ms (07:50:35.346)]
Trace[910390813]: [2.15538937s] [2.15538937s] END
I1229 07:50:38.360466       1 trace.go:236] Trace[2082989235]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d49dae02-b334-406a-bf88-2651e8aa50ba,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (29-Dec-2023 07:50:37.737) (total time: 622ms):
Trace[2082989235]: ["GuaranteedUpdate etcd3" audit-id:d49dae02-b334-406a-bf88-2651e8aa50ba,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 622ms (07:50:37.737)
Trace[2082989235]:  ---"Txn call completed" 613ms (07:50:38.360)]
Trace[2082989235]: [622.83411ms] [622.83411ms] END
I1229 07:50:38.362715       1 trace.go:236] Trace[1567798120]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/serviceips,type:*core.RangeAllocation,resource:serviceipallocations (29-Dec-2023 07:50:37.502) (total time: 860ms):
Trace[1567798120]: ---"initial value restored" 860ms (07:50:38.362)
Trace[1567798120]: [860.099228ms] [860.099228ms] END
I1229 07:50:38.384339       1 trace.go:236] Trace[1749900828]: "Update" accept:application/json, */*,audit-id:5019da5a-705d-4e13-b07e-01af64cdf99d,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (29-Dec-2023 07:50:37.774) (total time: 609ms):
Trace[1749900828]: ["GuaranteedUpdate etcd3" audit-id:5019da5a-705d-4e13-b07e-01af64cdf99d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 608ms (07:50:37.776)
Trace[1749900828]:  ---"About to Encode" 290ms (07:50:38.066)
Trace[1749900828]:  ---"Txn call completed" 316ms (07:50:38.382)]
Trace[1749900828]: [609.657067ms] [609.657067ms] END
I1229 07:50:38.397280       1 trace.go:236] Trace[1747757747]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:488f56e7-7f3c-4bcf-9aa7-d99eb3f57558,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:LIST (29-Dec-2023 07:50:37.501) (total time: 895ms):
Trace[1747757747]: ["List(recursive=true) etcd3" audit-id:488f56e7-7f3c-4bcf-9aa7-d99eb3f57558,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 895ms (07:50:37.501)]
Trace[1747757747]: [895.901625ms] [895.901625ms] END
I1229 07:50:38.566721       1 trace.go:236] Trace[1847389875]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (29-Dec-2023 07:50:37.502) (total time: 1064ms):
Trace[1847389875]: ---"initial value restored" 927ms (07:50:38.429)
Trace[1847389875]: ---"Txn call completed" 114ms (07:50:38.566)
Trace[1847389875]: [1.064409778s] [1.064409778s] END
I1229 07:51:01.679700       1 trace.go:236] Trace[1059662993]: "Update" accept:application/json, */*,audit-id:33bcce93-534f-42ab-b5bc-a2568df7b10a,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (29-Dec-2023 07:51:00.701) (total time: 978ms):
Trace[1059662993]: ["GuaranteedUpdate etcd3" audit-id:33bcce93-534f-42ab-b5bc-a2568df7b10a,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 977ms (07:51:00.702)
Trace[1059662993]:  ---"Txn call completed" 973ms (07:51:01.679)]
Trace[1059662993]: [978.195627ms] [978.195627ms] END
I1229 07:53:12.943905       1 trace.go:236] Trace[1722714679]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b531d15a-5ec8-41ef-986e-d8dce79736c9,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (29-Dec-2023 07:53:11.936) (total time: 1007ms):
Trace[1722714679]: ["GuaranteedUpdate etcd3" audit-id:b531d15a-5ec8-41ef-986e-d8dce79736c9,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1007ms (07:53:11.936)
Trace[1722714679]:  ---"Txn call completed" 1005ms (07:53:12.943)]
Trace[1722714679]: [1.007255958s] [1.007255958s] END
I1229 07:54:43.415522       1 trace.go:236] Trace[489066004]: "Get" accept:application/json, */*,audit-id:e338387d-b87e-4895-805e-80e8468bf042,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (29-Dec-2023 07:54:40.648) (total time: 2766ms):
Trace[489066004]: ---"About to write a response" 2766ms (07:54:43.415)
Trace[489066004]: [2.766898307s] [2.766898307s] END
I1229 07:55:01.897089       1 trace.go:236] Trace[1398469788]: "Get" accept:application/json, */*,audit-id:4f055e4c-be2d-418f-a430-cad0f6d0ee21,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (29-Dec-2023 07:55:01.018) (total time: 878ms):
Trace[1398469788]: ---"About to write a response" 877ms (07:55:01.896)
Trace[1398469788]: [878.082534ms] [878.082534ms] END
I1229 07:55:47.460765       1 alloc.go:330] "allocated clusterIPs" service="default/phpfpm-nginx" clusterIPs={"IPv4":"10.103.172.25"}
I1229 07:55:53.701414       1 trace.go:236] Trace[944722778]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3f885bab-f4d9-4506-be09-e56c1d84c8da,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (29-Dec-2023 07:55:52.776) (total time: 925ms):
Trace[944722778]: ["Create etcd3" audit-id:3f885bab-f4d9-4506-be09-e56c1d84c8da,key:/events/default/phpfpm-nginx-57dc94b95f-bbq62.17a53e51425e2a55,type:*core.Event,resource:events 922ms (07:55:52.778)
Trace[944722778]:  ---"Txn call succeeded" 922ms (07:55:53.701)]
Trace[944722778]: [925.026466ms] [925.026466ms] END
I1229 07:57:22.462959       1 trace.go:236] Trace[1797714131]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3648fca6-81bb-466d-afcf-f455dfb20b6c,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/phpfpm-nginx-57dc94b95f-csmqh.17a53e52c902ffd5,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (29-Dec-2023 07:57:21.863) (total time: 599ms):
Trace[1797714131]: ["GuaranteedUpdate etcd3" audit-id:3648fca6-81bb-466d-afcf-f455dfb20b6c,key:/events/default/phpfpm-nginx-57dc94b95f-csmqh.17a53e52c902ffd5,type:*core.Event,resource:events 585ms (07:57:21.876)
Trace[1797714131]:  ---"Txn call completed" 563ms (07:57:22.462)]
Trace[1797714131]: ---"About to check admission control" 15ms (07:57:21.898)
Trace[1797714131]: ---"Object stored in database" 564ms (07:57:22.462)
Trace[1797714131]: [599.807299ms] [599.807299ms] END

* 
* ==> kube-controller-manager [03d3cfa9ebeb] <==
* I1229 07:41:50.139007       1 range_allocator.go:178] "Starting range CIDR allocator"
I1229 07:41:50.139113       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1229 07:41:50.139151       1 shared_informer.go:318] Caches are synced for cidrallocator
I1229 07:41:50.161835       1 shared_informer.go:318] Caches are synced for deployment
I1229 07:41:50.168618       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1229 07:41:50.189120       1 shared_informer.go:318] Caches are synced for ReplicationController
I1229 07:41:50.190029       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I1229 07:41:50.168638       1 shared_informer.go:318] Caches are synced for daemon sets
I1229 07:41:50.168736       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1229 07:41:50.192319       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1229 07:41:50.169200       1 shared_informer.go:318] Caches are synced for taint
I1229 07:41:50.169225       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1229 07:41:50.203812       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1229 07:41:50.203926       1 taint_manager.go:211] "Sending events to api server"
I1229 07:41:50.172833       1 shared_informer.go:318] Caches are synced for endpoint
I1229 07:41:50.210331       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I1229 07:41:50.220494       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1229 07:41:50.228665       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1229 07:41:50.229014       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1229 07:41:50.229182       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I1229 07:41:50.237357       1 shared_informer.go:318] Caches are synced for resource quota
I1229 07:41:50.244562       1 shared_informer.go:318] Caches are synced for job
I1229 07:41:50.257894       1 shared_informer.go:318] Caches are synced for persistent volume
I1229 07:41:50.260622       1 shared_informer.go:318] Caches are synced for ephemeral
I1229 07:41:50.270121       1 shared_informer.go:318] Caches are synced for stateful set
I1229 07:41:50.271065       1 shared_informer.go:318] Caches are synced for cronjob
I1229 07:41:50.274157       1 shared_informer.go:318] Caches are synced for PVC protection
I1229 07:41:50.286534       1 shared_informer.go:318] Caches are synced for expand
I1229 07:41:50.294549       1 shared_informer.go:318] Caches are synced for TTL after finished
I1229 07:41:50.314077       1 shared_informer.go:318] Caches are synced for attach detach
I1229 07:41:50.342043       1 shared_informer.go:318] Caches are synced for resource quota
I1229 07:41:50.706328       1 shared_informer.go:318] Caches are synced for garbage collector
I1229 07:41:50.717625       1 shared_informer.go:318] Caches are synced for garbage collector
I1229 07:41:50.717691       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1229 07:41:50.918866       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I1229 07:41:51.112561       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-dbf9l"
I1229 07:41:51.186340       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-d7qq5"
I1229 07:41:51.283689       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="376.396758ms"
I1229 07:41:51.322043       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="37.855814ms"
I1229 07:41:51.322391       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="79.384Âµs"
I1229 07:41:51.352722       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="85.993Âµs"
I1229 07:41:54.192922       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="233.959Âµs"
I1229 07:41:54.510798       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="49.11272ms"
I1229 07:41:54.511341       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="455.173Âµs"
I1229 07:55:38.616972       1 event.go:307] "Event occurred" object="default/phpfpm-nginx" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set phpfpm-nginx-57dc94b95f to 3"
I1229 07:55:38.677278       1 event.go:307] "Event occurred" object="default/phpfpm-nginx-57dc94b95f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: phpfpm-nginx-57dc94b95f-54kf4"
I1229 07:55:38.695761       1 event.go:307] "Event occurred" object="default/phpfpm-nginx-57dc94b95f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: phpfpm-nginx-57dc94b95f-bbq62"
I1229 07:55:38.734249       1 event.go:307] "Event occurred" object="default/phpfpm-nginx-57dc94b95f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: phpfpm-nginx-57dc94b95f-csmqh"
I1229 07:55:38.850946       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="260.048474ms"
I1229 07:55:38.878899       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="27.779594ms"
I1229 07:55:38.880920       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="132.476Âµs"
I1229 07:55:38.899634       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="735.01Âµs"
I1229 07:55:38.934265       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="193.893Âµs"
I1229 07:56:32.589226       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="394.607Âµs"
I1229 07:56:36.746830       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="653.951Âµs"
I1229 07:56:41.944673       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="234.986Âµs"
I1229 07:56:59.489513       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="204.567Âµs"
I1229 07:57:04.492944       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="48.113Âµs"
I1229 07:57:06.485732       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="153.458Âµs"
I1229 07:57:27.489718       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/phpfpm-nginx-57dc94b95f" duration="104.758Âµs"

* 
* ==> kube-proxy [6c77293bdaf4] <==
* I1229 07:41:52.263949       1 server_others.go:69] "Using iptables proxy"
I1229 07:41:52.292444       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1229 07:41:52.358640       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1229 07:41:52.363469       1 server_others.go:152] "Using iptables Proxier"
I1229 07:41:52.363543       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1229 07:41:52.363554       1 server_others.go:438] "Defaulting to no-op detect-local"
I1229 07:41:52.363931       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1229 07:41:52.365029       1 server.go:846] "Version info" version="v1.28.3"
I1229 07:41:52.365106       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1229 07:41:52.368231       1 config.go:188] "Starting service config controller"
I1229 07:41:52.368781       1 shared_informer.go:311] Waiting for caches to sync for service config
I1229 07:41:52.368856       1 config.go:97] "Starting endpoint slice config controller"
I1229 07:41:52.368864       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1229 07:41:52.380079       1 config.go:315] "Starting node config controller"
I1229 07:41:52.380193       1 shared_informer.go:311] Waiting for caches to sync for node config
I1229 07:41:52.470152       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1229 07:41:52.470173       1 shared_informer.go:318] Caches are synced for service config
I1229 07:41:52.481261       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [5ecedb5a7b66] <==
* I1229 07:41:32.704016       1 serving.go:348] Generated self-signed cert in-memory
W1229 07:41:34.778094       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1229 07:41:34.778153       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1229 07:41:34.778168       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1229 07:41:34.778177       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1229 07:41:34.817551       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1229 07:41:34.819885       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1229 07:41:34.830955       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1229 07:41:34.831009       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W1229 07:41:34.838676       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1229 07:41:34.838854       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1229 07:41:34.844366       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1229 07:41:34.844875       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1229 07:41:34.892932       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1229 07:41:34.892999       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1229 07:41:34.893295       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1229 07:41:34.893559       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1229 07:41:34.898163       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1229 07:41:34.898232       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1229 07:41:34.898512       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1229 07:41:34.898633       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1229 07:41:34.900412       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1229 07:41:34.900722       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1229 07:41:34.901480       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1229 07:41:34.901548       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1229 07:41:34.901687       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1229 07:41:34.901815       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1229 07:41:34.902063       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1229 07:41:34.902115       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1229 07:41:34.902510       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1229 07:41:34.902561       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1229 07:41:34.902682       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1229 07:41:34.902830       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1229 07:41:34.902912       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1229 07:41:34.902926       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1229 07:41:34.905950       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1229 07:41:34.906226       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1229 07:41:34.909852       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1229 07:41:34.909882       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1229 07:41:34.911397       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1229 07:41:34.913077       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1229 07:41:35.843571       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1229 07:41:35.843634       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1229 07:41:36.427692       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1229 07:41:36.427774       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1229 07:41:38.532848       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.199093    2251 projected.go:198] Error preparing data for projected volume kube-api-access-xg66k for pod default/phpfpm-nginx-57dc94b95f-csmqh: failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.199801    2251 projected.go:292] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.199827    2251 projected.go:198] Error preparing data for projected volume kube-api-access-26cv8 for pod default/phpfpm-nginx-57dc94b95f-54kf4: failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.199864    2251 projected.go:292] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.199875    2251 projected.go:198] Error preparing data for projected volume kube-api-access-s4km2 for pod default/phpfpm-nginx-57dc94b95f-bbq62: failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.221086    2251 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/5b76d5d6-810e-412a-945a-2b0210dfc183-nginx-config-volume podName:5b76d5d6-810e-412a-945a-2b0210dfc183 nodeName:}" failed. No retries permitted until 2023-12-29 07:55:40.687265963 +0000 UTC m=+843.078772388 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "nginx-config-volume" (UniqueName: "kubernetes.io/configmap/5b76d5d6-810e-412a-945a-2b0210dfc183-nginx-config-volume") pod "phpfpm-nginx-57dc94b95f-54kf4" (UID: "5b76d5d6-810e-412a-945a-2b0210dfc183") : failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.235295    2251 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/0d9b5176-52a9-488f-bc4a-500fd943f898-nginx-config-volume podName:0d9b5176-52a9-488f-bc4a-500fd943f898 nodeName:}" failed. No retries permitted until 2023-12-29 07:55:40.735175077 +0000 UTC m=+843.126681483 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "nginx-config-volume" (UniqueName: "kubernetes.io/configmap/0d9b5176-52a9-488f-bc4a-500fd943f898-nginx-config-volume") pod "phpfpm-nginx-57dc94b95f-bbq62" (UID: "0d9b5176-52a9-488f-bc4a-500fd943f898") : failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.235337    2251 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/ed7a92ce-4223-41ca-b8d3-58faf5751cec-nginx-config-volume podName:ed7a92ce-4223-41ca-b8d3-58faf5751cec nodeName:}" failed. No retries permitted until 2023-12-29 07:55:40.735326038 +0000 UTC m=+843.126832434 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "nginx-config-volume" (UniqueName: "kubernetes.io/configmap/ed7a92ce-4223-41ca-b8d3-58faf5751cec-nginx-config-volume") pod "phpfpm-nginx-57dc94b95f-csmqh" (UID: "ed7a92ce-4223-41ca-b8d3-58faf5751cec") : failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.236497    2251 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ed7a92ce-4223-41ca-b8d3-58faf5751cec-kube-api-access-xg66k podName:ed7a92ce-4223-41ca-b8d3-58faf5751cec nodeName:}" failed. No retries permitted until 2023-12-29 07:55:40.735346649 +0000 UTC m=+843.126853045 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-xg66k" (UniqueName: "kubernetes.io/projected/ed7a92ce-4223-41ca-b8d3-58faf5751cec-kube-api-access-xg66k") pod "phpfpm-nginx-57dc94b95f-csmqh" (UID: "ed7a92ce-4223-41ca-b8d3-58faf5751cec") : failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.236569    2251 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/5b76d5d6-810e-412a-945a-2b0210dfc183-kube-api-access-26cv8 podName:5b76d5d6-810e-412a-945a-2b0210dfc183 nodeName:}" failed. No retries permitted until 2023-12-29 07:55:40.736544644 +0000 UTC m=+843.128051040 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-26cv8" (UniqueName: "kubernetes.io/projected/5b76d5d6-810e-412a-945a-2b0210dfc183-kube-api-access-26cv8") pod "phpfpm-nginx-57dc94b95f-54kf4" (UID: "5b76d5d6-810e-412a-945a-2b0210dfc183") : failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:40 minikube kubelet[2251]: E1229 07:55:40.236594    2251 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/0d9b5176-52a9-488f-bc4a-500fd943f898-kube-api-access-s4km2 podName:0d9b5176-52a9-488f-bc4a-500fd943f898 nodeName:}" failed. No retries permitted until 2023-12-29 07:55:40.736581773 +0000 UTC m=+843.128088179 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-s4km2" (UniqueName: "kubernetes.io/projected/0d9b5176-52a9-488f-bc4a-500fd943f898-kube-api-access-s4km2") pod "phpfpm-nginx-57dc94b95f-bbq62" (UID: "0d9b5176-52a9-488f-bc4a-500fd943f898") : failed to sync configmap cache: timed out waiting for the condition
Dec 29 07:55:46 minikube kubelet[2251]: I1229 07:55:46.646987    2251 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="31cfcfc46e30108e65128016a0951d2d603fbb66bceea2af644ec3bbb75db149"
Dec 29 07:55:46 minikube kubelet[2251]: I1229 07:55:46.750508    2251 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b6a5a0569d4997ebb98d513cc995d6e9cd4486dcad5ab3816ef0afff25bf2913"
Dec 29 07:55:46 minikube kubelet[2251]: I1229 07:55:46.754366    2251 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c4f31dfc382092382a0f6047fc0db3ed002b9281e8ec30766d13e0b9574f92c5"
Dec 29 07:55:52 minikube kubelet[2251]: E1229 07:55:52.754896    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:55:52 minikube kubelet[2251]: E1229 07:55:52.755767    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:55:52 minikube kubelet[2251]: E1229 07:55:52.757752    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-s4km2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-bbq62_default(0d9b5176-52a9-488f-bc4a-500fd943f898): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:55:53 minikube kubelet[2251]: E1229 07:55:53.675805    2251 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.055s"
Dec 29 07:55:59 minikube kubelet[2251]: E1229 07:55:59.309832    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:55:59 minikube kubelet[2251]: E1229 07:55:59.309975    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:55:59 minikube kubelet[2251]: E1229 07:55:59.310352    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-xg66k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-csmqh_default(ed7a92ce-4223-41ca-b8d3-58faf5751cec): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:56:03 minikube kubelet[2251]: E1229 07:56:03.649797    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:03 minikube kubelet[2251]: E1229 07:56:03.649874    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:03 minikube kubelet[2251]: E1229 07:56:03.650112    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-26cv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-54kf4_default(5b76d5d6-810e-412a-945a-2b0210dfc183): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:56:22 minikube kubelet[2251]: E1229 07:56:22.147038    2251 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.649s"
Dec 29 07:56:32 minikube kubelet[2251]: E1229 07:56:32.315951    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-bbq62" podUID="0d9b5176-52a9-488f-bc4a-500fd943f898"
Dec 29 07:56:36 minikube kubelet[2251]: E1229 07:56:36.241762    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-csmqh" podUID="ed7a92ce-4223-41ca-b8d3-58faf5751cec"
Dec 29 07:56:38 minikube kubelet[2251]: W1229 07:56:38.452952    2251 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 29 07:56:40 minikube kubelet[2251]: E1229 07:56:40.854011    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-54kf4" podUID="5b76d5d6-810e-412a-945a-2b0210dfc183"
Dec 29 07:56:45 minikube kubelet[2251]: E1229 07:56:45.250005    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:45 minikube kubelet[2251]: E1229 07:56:45.250132    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:45 minikube kubelet[2251]: E1229 07:56:45.250568    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-s4km2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-bbq62_default(0d9b5176-52a9-488f-bc4a-500fd943f898): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:56:45 minikube kubelet[2251]: E1229 07:56:45.250674    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-bbq62" podUID="0d9b5176-52a9-488f-bc4a-500fd943f898"
Dec 29 07:56:46 minikube kubelet[2251]: E1229 07:56:46.059114    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-bbq62" podUID="0d9b5176-52a9-488f-bc4a-500fd943f898"
Dec 29 07:56:50 minikube kubelet[2251]: E1229 07:56:50.254868    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:50 minikube kubelet[2251]: E1229 07:56:50.255780    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:50 minikube kubelet[2251]: E1229 07:56:50.256212    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-xg66k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-csmqh_default(ed7a92ce-4223-41ca-b8d3-58faf5751cec): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:56:50 minikube kubelet[2251]: E1229 07:56:50.256553    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-csmqh" podUID="ed7a92ce-4223-41ca-b8d3-58faf5751cec"
Dec 29 07:56:51 minikube kubelet[2251]: E1229 07:56:51.156158    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-csmqh" podUID="ed7a92ce-4223-41ca-b8d3-58faf5751cec"
Dec 29 07:56:54 minikube kubelet[2251]: E1229 07:56:54.656709    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:54 minikube kubelet[2251]: E1229 07:56:54.671666    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:56:54 minikube kubelet[2251]: E1229 07:56:54.671766    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-26cv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-54kf4_default(5b76d5d6-810e-412a-945a-2b0210dfc183): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:56:54 minikube kubelet[2251]: E1229 07:56:54.671807    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-54kf4" podUID="5b76d5d6-810e-412a-945a-2b0210dfc183"
Dec 29 07:56:55 minikube kubelet[2251]: E1229 07:56:55.391874    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-54kf4" podUID="5b76d5d6-810e-412a-945a-2b0210dfc183"
Dec 29 07:56:59 minikube kubelet[2251]: E1229 07:56:59.444010    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-bbq62" podUID="0d9b5176-52a9-488f-bc4a-500fd943f898"
Dec 29 07:57:04 minikube kubelet[2251]: E1229 07:57:04.444945    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-csmqh" podUID="ed7a92ce-4223-41ca-b8d3-58faf5751cec"
Dec 29 07:57:06 minikube kubelet[2251]: E1229 07:57:06.452463    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-54kf4" podUID="5b76d5d6-810e-412a-945a-2b0210dfc183"
Dec 29 07:57:16 minikube kubelet[2251]: E1229 07:57:16.582905    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:57:16 minikube kubelet[2251]: E1229 07:57:16.582985    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:57:16 minikube kubelet[2251]: E1229 07:57:16.583126    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-s4km2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-bbq62_default(0d9b5176-52a9-488f-bc4a-500fd943f898): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:57:16 minikube kubelet[2251]: E1229 07:57:16.585118    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-bbq62" podUID="0d9b5176-52a9-488f-bc4a-500fd943f898"
Dec 29 07:57:21 minikube kubelet[2251]: E1229 07:57:21.850798    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:57:21 minikube kubelet[2251]: E1229 07:57:21.850862    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:57:21 minikube kubelet[2251]: E1229 07:57:21.854083    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-xg66k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-csmqh_default(ed7a92ce-4223-41ca-b8d3-58faf5751cec): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:57:21 minikube kubelet[2251]: E1229 07:57:21.854450    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-csmqh" podUID="ed7a92ce-4223-41ca-b8d3-58faf5751cec"
Dec 29 07:57:27 minikube kubelet[2251]: E1229 07:57:27.012314    2251 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:57:27 minikube kubelet[2251]: E1229 07:57:27.013474    2251 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-php-app:1.0.0"
Dec 29 07:57:27 minikube kubelet[2251]: E1229 07:57:27.021585    2251 kuberuntime_manager.go:1256] container &Container{Name:app,Image:my-php-app:1.0.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:shared-files,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-26cv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:&Lifecycle{PostStart:&LifecycleHandler{Exec:&ExecAction{Command:[/bin/sh -c cp -r /app/. /var/www/html],},HTTPGet:nil,TCPSocket:nil,},PreStop:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod phpfpm-nginx-57dc94b95f-54kf4_default(5b76d5d6-810e-412a-945a-2b0210dfc183): ErrImagePull: Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 29 07:57:27 minikube kubelet[2251]: E1229 07:57:27.021789    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-php-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/phpfpm-nginx-57dc94b95f-54kf4" podUID="5b76d5d6-810e-412a-945a-2b0210dfc183"
Dec 29 07:57:27 minikube kubelet[2251]: E1229 07:57:27.440887    2251 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-php-app:1.0.0\\\"\"" pod="default/phpfpm-nginx-57dc94b95f-bbq62" podUID="0d9b5176-52a9-488f-bc4a-500fd943f898"

* 
* ==> storage-provisioner [2cac54d656be] <==
* I1229 07:41:53.041226       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1229 07:41:53.123237       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1229 07:41:53.125880       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1229 07:41:53.165251       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1229 07:41:53.174038       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e7a4ceae-7f6b-4afa-923a-357e8ffd1ed8!
I1229 07:41:53.171927       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"06d69a07-b975-45bd-8113-0b266ce0707d", APIVersion:"v1", ResourceVersion:"398", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e7a4ceae-7f6b-4afa-923a-357e8ffd1ed8 became leader
I1229 07:41:53.279142       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e7a4ceae-7f6b-4afa-923a-357e8ffd1ed8!

* 
* ==> storage-provisioner [39348c97d798] <==
* I1229 07:41:52.036708       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1229 07:41:52.059593       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

